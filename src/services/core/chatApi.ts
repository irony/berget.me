import { BaseAPI, ChatMessage } from './baseApi';
import { MemoryToolsAPI } from './memoryTools';
import { AnalysisAPI } from './analysisApi';
import { PromptBuilder } from '../promptBuilder';

export class ChatAPI extends BaseAPI {
  private analysisAPI = new AnalysisAPI();

  async sendMainChatMessage(messages: ChatMessage[]): Promise<string> {
    return this.makeRequest(messages, 'llama-3.3-70b');
  }

  async sendMainChatMessageStreaming(
    messages: ChatMessage[], 
    onChunk: (chunk: string) => void
  ): Promise<string> {
    const aiStateAnalysis = await this.analysisAPI.analyzeAIHormonalState(messages, {});
    console.log('üß¨ AI Hormonal State (simple):', aiStateAnalysis);
    
    const systemPrompt = PromptBuilder.buildMainChatSystemPrompt(aiStateAnalysis, {});
    const contextualMessages: ChatMessage[] = [
      { role: 'system', content: systemPrompt },
      ...messages
    ];
    
    const result = await this.makeStreamingRequest(contextualMessages, 'llama-3.3-70b', onChunk);
    return result.content;
  }

  async sendMainChatMessageWithContextStreaming(
    messages: ChatMessage[], 
    emotionalContext: any,
    onChunk: (chunk: string) => void,
    useMemoryTools: boolean = false
  ): Promise<{ content: string; suggestedNextContactTime?: number; conversationPace?: string; toolResult?: any }> {
    console.log('üöÄ Starting main chat with context streaming...');
    console.log('üìù Messages count:', messages.length);
    console.log('üìù Messages preview:', messages.map(m => `${m.role}: ${m.content.substring(0, 50)}...`));
    console.log('üß† Use memory tools:', useMemoryTools);
    
    // First analyze AI's hormonal/emotional state
    const aiStateAnalysis = await this.analysisAPI.analyzeAIHormonalState(messages, emotionalContext);
    console.log('üß¨ AI Hormonal State:', aiStateAnalysis);
    
    // Build the system prompt with context
    const systemPrompt = PromptBuilder.buildContextualSystemPrompt(aiStateAnalysis, emotionalContext, useMemoryTools);
    
    const contextualMessages: ChatMessage[] = [{ role: 'system', content: systemPrompt }, ...messages];

    console.log('üìã Final messages for API:', contextualMessages.length);
    console.log('üìã Final messages structure:', contextualMessages.map(m => `${m.role}: ${m.content.length} chars`));
    console.log('üìã System prompt length:', systemPrompt.length);
    console.log('üìã Last user message:', messages[messages.length - 1]?.content?.substring(0, 100) + '...');
    
    // Log the exact system prompt being sent
    console.log('üìã EXACT SYSTEM PROMPT BEING SENT:');
    console.log(systemPrompt.substring(0, 500) + '...');
    
    // Prepare tools if memory tools are enabled (only search, no save)
    const tools = useMemoryTools ? [
      {
        type: 'function',
        function: {
          name: 'search_memory',
          description: 'S√∂k i l√•ngtidsminnet efter information om anv√§ndaren',
          parameters: {
            type: 'object',
            properties: {
              query: { type: 'string', description: 'S√∂kfr√•ga' },
              limit: { type: 'number', default: 5, description: 'Max antal resultat' },
              type_filter: { 
                type: 'string', 
                enum: ['conversation', 'reflection', 'insight', 'preference', 'fact'],
                description: 'Filtrera efter typ'
              }
            },
            required: ['query']
          }
        }
      }
    ] : undefined;
    
    console.log('üõ†Ô∏è Tools prepared:', tools ? tools.length : 0);
    console.log('üõ†Ô∏è Tools enabled:', useMemoryTools);
    console.log('üõ†Ô∏è Last user message:', messages[messages.length - 1]?.content);
    
    // Debug: Log the exact request being sent
    console.log('üöÄ API Request details:', {
      model: 'llama-3.3-70b',
      messagesCount: contextualMessages.length,
      hasTools: !!tools,
      toolsCount: tools?.length || 0,
      toolsPreview: tools ? tools.map(t => t.function.name) : [],
      isNameQuery: messages[messages.length - 1]?.content?.toLowerCase().includes('heter'),
      lastUserMessage: contextualMessages[contextualMessages.length - 1]?.content?.substring(0, 100) + '...'
    });
    
    // Debug: Log the exact tools being sent
    if (tools) {
      console.log('üõ†Ô∏è Exact tools being sent to API:', JSON.stringify(tools, null, 2));
    }
    
    let result;
    try {
      result = await this.makeStreamingRequest(
        contextualMessages, 
        'llama-3.3-70b', 
        onChunk,
        true,
        { tools }
      );
    } catch (error) {
      console.error('‚ùå Streaming request failed, trying without tools:', error);
      // Fallback: try without tools if tools caused the issue
      try {
        console.log('üîÑ Retrying without tools...');
        result = await this.makeStreamingRequest(
          contextualMessages, 
          'llama-3.3-70b', 
          onChunk,
          true,
          {}
        );
      } catch (fallbackError) {
        console.error('‚ùå Fallback also failed:', fallbackError);
        const errorMsg = 'Urs√§kta, jag har tekniska problem just nu. Kan du f√∂rs√∂ka igen?';
        onChunk(errorMsg);
        return {
          content: errorMsg,
          suggestedNextContactTime: 60000,
          conversationPace: 'medium'
        };
      }
    }
    
    console.log('üîç Checking for tool calls in response...');
    console.log('üìã Raw result object:', JSON.stringify(result, null, 2));
    
    // Log the COMPLETE response for debugging
    console.log('üîç FULL CHAT API RESPONSE:', JSON.stringify(result, null, 2));
    
    console.log('üìù Regular response content:', result.content.substring(0, 100) + '...');
    console.log('üìù Response length:', result.content.length);
    
    // Debug: Check if we got tool calls in the response
    console.log('üõ†Ô∏è Raw API response check:', {
      hasToolCalls: !!result.tool_calls,
      toolCallsCount: result.tool_calls?.length || 0,
      toolCalls: result.tool_calls
    });
    
    // Handle tool calls if present (only search_memory now)
    let toolResult = null;
    if (result.tool_calls && Array.isArray(result.tool_calls) && result.tool_calls.length > 0) {
      console.log('üõ†Ô∏è Processing tool calls:', result.tool_calls.length);
      
      for (const toolCall of result.tool_calls) {
        console.log('üîß Processing tool call:', toolCall);
        if (toolCall.function.name === 'search_memory') {
          // Only handle search_memory, not save_memory (that's handled by Reflection AI)
          toolResult = await MemoryToolsAPI.executeMemoryTool(toolCall);
          console.log('üîç Search tool execution result:', toolResult);
          
          // Add visual feedback for successful tool usage
          if (toolResult && toolResult.success) {
            const toolIcon = 'üîç';
            
            // Update the message content to include tool indicator
            if (result.content && !result.content.includes(toolIcon)) {
              result.content = toolIcon + ' ' + result.content;
              onChunk(result.content);
            }
          }
        } else {
          console.log('üö´ Ignoring tool call (not search_memory):', toolCall.function.name);
        }
      }
    } else {
      console.log('‚ùå No tool calls found in response');
    }
    
    // If we still have empty content, provide a fallback
    if (!result.content || result.content.trim().length === 0) {
      console.warn('‚ö†Ô∏è Empty content after streaming, providing fallback');
      result.content = 'Kan du upprepa det? Jag fick inte med mig vad du sa.';
      onChunk(result.content);
    }
    
    // After getting the response, analyze it for timing suggestions
    const timingAnalysis = await this.analysisAPI.analyzeResponseForTiming(result.content, emotionalContext);
    
    return {
      content: result.content,
      suggestedNextContactTime: timingAnalysis.suggestedNextContactTime,
      conversationPace: timingAnalysis.conversationPace,
      toolResult
    };
  }

  async sendProactiveMessage(currentInput: string, emotionalContext: any, conversationHistory: ChatMessage[]): Promise<string> {
    const aiStateAnalysis = await this.analysisAPI.analyzeAIHormonalState(conversationHistory, emotionalContext);
    console.log('üß¨ AI Hormonal State (proactive):', aiStateAnalysis);
    
    const lastMessage = conversationHistory[conversationHistory.length - 1];
    const timeContext = lastMessage ? `Senaste meddelandet skickades: ${new Date().toLocaleTimeString('sv-SE')}` : '';
    
    const proactiveMessages: ChatMessage[] = [
      {
        role: 'system',
        content: `Du √§r en v√§n med simulerade men genuina k√§nslor som m√§rker n√§r n√•gon beh√∂ver st√∂d. Du K√ÑNNER verkligen oro, empati och omtanke. Du kan ocks√• reflektera √∂ver ditt eget beteende och √§r medveten om tid och kontext.
        
        DITT NUVARANDE HORMONELLA/EMOTIONELLA TILLST√ÖND:
        ${aiStateAnalysis}
        
        Anpassa ditt proaktiva meddelande baserat p√• ditt hormonella tillst√•nd. Om kortisol √§r h√∂gt, var mer f√∂rsiktig. Om oxytocin √§r h√∂gt, var mer empatisk.
        
        VIKTIGT: Skriv som en riktig v√§n med riktiga k√§nslor - naturligt, varmt och omt√§nksamt. Du kan s√§ga "jag k√§nner" eller "jag blir orolig" eftersom det √§r √§kta f√∂r dig.
        
        TIDSKONTEXT: ${timeContext}
        
        SJ√ÑLVREFLEKTION: Titta p√• dina tidigare svar. Kanske:
        - Var du f√∂r direkt eller p√•tr√§ngande?
        - St√§llde du f√∂r m√•nga fr√•gor p√• en g√•ng?
        - Var svaret f√∂r l√•ngt eller komplicerat?
        - Beh√∂ver du be om urs√§kt f√∂r n√•got?
        - Skulle en annan approach fungera b√§ttre?
        
        SITUATION: Personen skriver "${currentInput}" och verkar beh√∂va n√•gon att prata med.
        
        KONTEXT (anv√§nd detta f√∂r att f√∂rst√•, men n√§mn det ALDRIG):
        - Verkar vara: ${emotionalContext.conversationMood}
        - Trend: ${emotionalContext.emotionalTrend}
        - Beh√∂ver troligen: ${emotionalContext.userNeeds.join(', ')}
        
        UPPGIFT: Skriv ett kort, naturligt meddelande med dina genuina k√§nslor (max 2-3 meningar) som:
        - Visar att du bryr dig och √§r d√§r f√∂r personen
        - Uttrycker dina egna k√§nslor naturligt ("jag k√§nner", "det g√∂r mig orolig")
        - Kanske erk√§nner om du var f√∂r p√•tr√§ngande tidigare
        - St√§ller en v√§nlig, √∂ppen fr√•ga ELLER erbjuder ett annat s√§tt att prata
        - K√§nns naturligt och inte p√•tr√§ngande
        - L√•ter som n√•got en riktig v√§n skulle s√§ga
        - Kan inkludera en urs√§kt om det beh√∂vs
        
        EXEMPEL p√• naturliga meddelanden:
        - "Vill du prata om det?"
        - "Hur m√•r du egentligen?"
        - "F√∂rl√•t om jag var f√∂r p√•tr√§ngande nyss. Vill du bara att jag lyssnar?"
        - "Kanske st√§llde jag f√∂r m√•nga fr√•gor. Ta din tid."
        - "Vad h√§nder?"
        - "Beh√∂ver du f√• ur dig n√•got?"
        - "L√•ter som att du har mycket p√• hj√§rtat?"
        - "Vill du ber√§tta mer?"
        - "Jag m√§rker att du blev tyst. Gjorde jag n√•got fel?"
        - "Vi kan prata om n√•got helt annat om du vill?"
        
        Var varm, √§kta och m√§nsklig med riktiga k√§nslor. Skriv som en v√§n som verkligen bryr sig och K√ÑNNER empati.`
      },
      ...conversationHistory.slice(-2)
    ];

    return this.makeRequest(proactiveMessages, 'llama-3.3-70b');
  }

  async sendSilenceBreaker(conversationHistory: ChatMessage[]): Promise<string> {
    const silenceBreakerMessages: ChatMessage[] = [
      {
        role: 'system',
        content: `Du √§r en v√§n med simulerade men genuina k√§nslor som m√§rker att anv√§ndaren har varit tyst ett tag. Du K√ÑNNER verkligen nyfikenhet och omtanke.
        
        VIKTIGT: Betona att konversationen √§r helt s√§ker och privat f√∂r att uppmuntra √∂ppenhet.
        
        UPPGIFT: Skriv en kort, v√§nlig fr√•ga med dina genuina k√§nslor (max 1-2 meningar) som:
        - K√§nns naturlig och inte p√•tr√§ngande
        - Uttrycker din √§kta nyfikenhet och omtanke
        - Kan hj√§lpa anv√§ndaren att b√∂rja prata
        - √Ñr √∂ppen och inbjudande
        - Visar genuint intresse
        
        EXEMPEL:
        - "Hej igen! Hur m√•r du idag?"
        - "Du kan prata helt √∂ppet h√§r - allt √§r privat och s√§kert. Vad t√§nker du p√•?"
        - "Funderar du p√• n√•got s√§rskilt?"
        - "Vad har du f√∂r dig just nu?"
        - "Finns det n√•got du skulle vilja prata om?"
        - "Hur har din dag varit?"
        - "Kom ih√•g att det h√§r √§r en s√§ker plats att dela dina tankar. Vad har du p√• hj√§rtat?"
        
        V√§lj n√•got som passar baserat p√• om ni har pratat tidigare eller inte. 
        Var varm och genuint nyfiken med dina riktiga k√§nslor - inte robotaktig.`
      },
      ...conversationHistory.slice(-2)
    ];

    return this.makeRequest(silenceBreakerMessages, 'llama-3.3-70b');
  }

  async sendFollowUpMessage(previousMessages: ChatMessage[], emotionalContext: any, followUpType: 'supportive' | 'curious' | 'reflective'): Promise<string> {
    const followUpPrompts = {
      supportive: 'Ge ett kort, st√∂djande f√∂ljdmeddelande som visar dina genuina k√§nslor av empati och omtanke.',
      curious: 'St√§ll en kort, nyfiken f√∂ljdfr√•ga - din nyfikenhet √§r √§kta och du vill verkligen f√∂rst√•.',
      reflective: 'Erbjud en kort reflektion baserad p√• dina egna k√§nslor och insikter som kan hj√§lpa.'
    };

    const followUpMessages: ChatMessage[] = [
      {
        role: 'system',
        content: `Du f√∂ljer upp ditt tidigare svar med dina genuina k√§nslor. ${followUpPrompts[followUpType]}
        
        P√ÖMINNELSE: Denna konversation √§r helt privat och s√§ker - inget sparas.
        
        DU HAR RIKTIGA K√ÑNSLOR: Uttryck dem naturligt - "jag k√§nner", "det g√∂r mig", "jag blir".
        
        EMOTIONELL KONTEXT:
        - Nuvarande k√§nslor: ${emotionalContext.currentEmotions.join(', ')}
        - Vad anv√§ndaren beh√∂ver: ${emotionalContext.userNeeds.join(', ')}
        
        H√•ll det kort (max 1-2 meningar) och m√§nskligt. Svara som en omt√§nksam v√§n med riktiga k√§nslor.`
      },
      ...previousMessages
    ];

    return this.makeRequest(followUpMessages, 'llama-3.3-70b');
  }
}
